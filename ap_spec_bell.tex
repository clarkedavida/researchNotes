\chapter{Special Topic: Bell's Theorem}\label{ap:bell}

This chapter gives a brief introduction to Bell's theorem and its role in
interpreting quantum mechanics (QM). The presentation in the first two sections
is based mostly off the one given by Griffiths 
\cite{griffiths_introduction_2005}. In the last section, I wanted to focus
on one of the assumptions necessary to derive Bell's inequality because
it's fun to think about philosophically in the context of free will.

Let's recall what happens when you make a measurement in a quantum system.
Generically speaking, you have a system whose state is given by a wave
function. This wave function doesn't tell you exactly what the
outcome of a measurement will be, but rather is related to a statistical
distribution of possible outcomes. Only after you make a measurement do
you know the unique state definitively, and we say the wave function collapses.

But one may wonder whether this distribution merely stands in for some
hidden knowledge that we don't have. For example when one throws a die
in real life, one could, at least in principle, predict the outcome of
the die throw, using classical mechanics, given some initial conditions.
In practice we just say each face is equally likely and leave it at that.
In QM, this viewpoint that there must be some attribute
of the system that tells us what the measurement should be is called
the {\it realist} viewpoint. The viewpoint that the act of measurement
somehow ``creates" this attribute is the {\it orthodox} viewpoint.
Of course one may feel that answering such a question falls outside the
scope of physics entirely; this is the {\it agnostic} viewpoint.

The orthodox view is the textbook interpretation of quantum
measurements. Formally, the observable of interest does not have a
well-defined value before measurement. This has lead to a powerful,
consistent, predictive, successful theory. But it might make one 
uncomfortable\footnote{It is important to emphasize that discomfort
with a theory says nothing about its truth. But it can be enough of
a motivation to at least consider other alternatives.} to
think that the universe is fundamentally truly stochastic, and it
may also seem strange that the act of measuring has such a special
position in the theory. A more modern motivation to re-examine this
viewpoint is that there are many serious, unsolved problems\footnote{In
particular I have in mind that quantum field theories and gravity are
not yet unified.} in physics today,
and one may wonder whether these problems could be treated in a
more complete theory that also explains these ambiguities.

\section{The EPR paradox}
Actually physicists have been uncomfortable with QM for a long time.
In 1935 Einstein, Podolsky, and Rosen
\cite{einstein_can_1935} published the {\it EPR paradox},
which was designed to show that the realist viewpoint must be
the correct one. The following variant of the EPR paradox seems to be
rather popular, and is due to Bohm \cite{bohm_quantum_1951}.
Consider the decay of a neutral pion to an electron and positron:
\begin{equation}
  \pi^0\to e^-+e^+.
\end{equation}
If the pion was at rest, the $e^+$ and $e^-$ will fly off in opposite
directions with the same speed. Since the pion is spin-0, we know
by conservation of angular momentum that the RHS is in the
singlet configuration, i.e.
\begin{equation}
  \ket{\psi}=\frac{1}{\sqrt{2}}\big(
   \ket{\uparrow\downarrow}-\ket{\downarrow\uparrow}\big),
\end{equation}
where the first and second components of the ket indicate the spins of
the electron and positron, respectively.
Since the system is in the singlet configuration, if an experimenter
measures the spin of the positron, they know the spin of the electron
immediately, and vice-versa, regardless of how separated they are.
To reference this phenomenon, one sometimes speaks of {\it entanglement}.

This is at least superficially surprising from the orthodox perspective.
The reason is that special relativity tells us no influence can travel
faster than light, i.e. our theory must be {\it local}; 
otherwise it would violate causality\footnote{This reasoning will need to
be examined more carefully in Section~\ref{sec:interpretingBell}.}. 
On the other
hand, it must be that wave function collapse is instantaneous; otherwise
one would, at least momentarily, violate angular momentum conservation.
Finally, entangled particles are experimentally known to have perfectly
correlated spins. 

Einstein, Podolsky, and Rosen therefore conclude
that the orthodox opinion is untenable. One can then suppose there is
some kind of missing information, or {\it hidden variable}, which
is in these discussions often labeled $\lambda$. The hidden variable
could be a single number, but it could also be a collection
of numbers. We think of $\lambda$ as some kind of complete specification of
the system at its source.

\section{Bell's inequality}
In 1964, Bell proved that, given some very mild, highly intuitive assumptions,
all local hidden variable theories are incompatible with QM
\cite{bell_einstein_1964}.

Bell proceeds as follows: We start with a generalization of Bohm's version
of the EPR setup. A neutral pion decays into an electron and a positron.
One experimenter measures the spin of the electron along the vector
$\vec{a}$ and gets result $A$; the other measures the spin of the positron
along vector $\vec{b}$ and gets result $B$. For simplicity we will record
the spins in units of $\hbar/2$, which means $A$, $B\in\{-1,1\}$. We
calculate the average of the product of these spins, which we label 
$P_{\vec{a},\vec{b}}$.
QM predicts
\begin{equation}\label{eq:bellQM}
  P_{\vec{a},\vec{b}}=-\vec{a}\cdot\vec{b}.
\end{equation}

Equation~\eqref{eq:bellQM} is what we expect with the theory as is, i.e. with
no hidden variables.
We will now suppose there exists some generic hidden variable $\lambda$ and see
what we find. If there were a hidden variable, then $A$ and $B$ could now
be considered functions\footnote{In ordinary QM, the $A$ and $B$ are
purely stochastic.} that depend
on $\lambda$, $\vec{a}$, and $\vec{b}$. Further suppose
that $A$ is independent of the setting $\vec{b}$ and
vice-versa\footnote{This assumption is reasonable, since the setting 
$\vec{b}$ can
in principle be chosen just before the measurement $A$ is made, and therefore
before any influence from the electron side can propagate to the positron side.
This independence assumption can be circumvented, for example by considering
an influence in the distant past of both the electron and positron's
backwards light cones. See Section~\ref{sec:interpretingBell}.}; this
assumption is sometimes called {\it independence}. With this modest
assumption, we can write
\begin{equation}\label{eq:bellindep}
  A(\vec{a},\lambda)=\pm1~~~~\text{and}~~~~B(\vec{b},\lambda)=\pm1,
\end{equation}
and when the detectors are aligned, we should have by angular momentum 
conservation
\begin{equation}
  A(\vec{a},\lambda)=-B(\vec{a},\lambda).
\end{equation}

Let us now compute the average $P_{\vec{a},\vec{b}}$. We don't know 
exactly what $\lambda$ is, so our strategy will be to integrate over all 
possibilities for $\lambda$
could be, with a density $\rho(\lambda)$ controlling the system's propensity
to take the particular value $\lambda$. In this language, impossible
$\lambda$ have measure zero. We know that $\lambda$ must be allowed to
change between experiments, because if perform the experiment ``measure $A$"
twice, I will not in general get the same result twice. Since $\rho$ is to
be interpreted as a frequency, it is non-negative\footnote{You calculate
averages by adding together results, not by subtracting some.}.
Moreover we can choose $\rho$ to be normalized WLOG. Thus
\begin{equation}\label{eq:belldensity}
  \rho(\lambda)\geq 0~~~~\text{and}~~~~\int\dd{\lambda}\rho(\lambda)=1.
\end{equation}
Then
\begin{equation}\begin{aligned}
  P_{\vec{a},\vec{b}}
         &=\int\dd{\lambda}\rho(\lambda)A(\vec{a},\lambda)B(\vec{b},\lambda)\\
         &=-\int\dd{\lambda}\rho(\lambda)A(\vec{a},\lambda)A(\vec{b},\lambda).
\end{aligned}\end{equation}

We now have all necessary information to derive the {\it Bell inequality}. 
If $\vec{c}$ is any other vector, we obtain
\begin{equation}\begin{aligned}
  P_{\vec{a},\vec{b}}-P_{\vec{a},\vec{c}}
   &=-\int\dd{\lambda}\rho(\lambda)
      \left[A(\vec{a},\lambda)A(\vec{b},\lambda)-
            A(\vec{a},\lambda)A(\vec{c},\lambda)\right]\\
   &=-\int\dd{\lambda}\rho(\lambda)
      A(\vec{a},\lambda)A(\vec{b},\lambda)
      \left[1-A(\vec{b},\lambda)A(\vec{c},\lambda)\right]
\end{aligned}\end{equation}
since $A(\vec{b},\lambda)^2=1$. From eqs.~\eqref{eq:bellindep} and
\eqref{eq:belldensity} we know that
\begin{equation}
  |A(\vec{a},\lambda)A(\vec{b},\lambda)|=1~~~~\text{and}~~~~
  \rho(\lambda)\left[1-A(\vec{b},\lambda)A(\vec{c},\lambda)\right]\geq0,
\end{equation}
and it is a general property of integrable functions on generic
measure spaces that
\begin{equation}
  \left|\int\dd{\mu}f(\mu)\right|\leq\int\dd{\mu}|f(\mu)|.
\end{equation}
Hence we can derive the bound
\begin{equation}
  |P_{\vec{a},\vec{b}}-P_{\vec{a},\vec{c}}\,|
  \leq\int\dd{\lambda}\left[1-A(\vec{b},\lambda)A(\vec{c},\lambda)\right].
\end{equation}
Therefore
\begin{theorem}{Bell's inequality}{}
  $$
  |P_{\vec{a},\vec{b}}-P_{\vec{a},\vec{c}}\,|
  \leq1+P_{\,\vec{b},\vec{c}}.
  $$
\end{theorem}

One can immediately see that Bell's inequality is incompatible with
the QM prediction eq.~\eqref{eq:bellQM}. For suppose $\vec{a}$, $\vec{b}$,
and $\vec{c}$ all lie in a plane in such a way that $\vec{a}$ and $\vec{b}$
are orthogonal with $\vec{c}$ at an angle $\pi/4$ half way between
$\vec{a}$ and $\vec{b}$. Then QM tells us
\begin{equation}
  P_{\vec{a},\vec{b}}=0~~~~\text{and}~~~~
  P_{\vec{a},\vec{c}}=P_{\,\vec{b},\vec{c}}\approx-0.707,
\end{equation}
which violates Bell's inequality, since
\begin{equation}
  0.707\nleq1-0.707=0.293.
\end{equation}
This suggests something rather strong: either all local hidden variable 
theories satisfying independence are wrong, or else QM is.

To determine who is correct, one can carry out simple (at least conceptually
simple) experiments to test Bell's inequality. One of the most famous
is the experiment of Aspect, Grangier, and Roger
\cite{aspect_experimental_1982}, which verifies the QM prediction. 

\section{Reconciliation}\label{sec:interpretingBell}

How can we reconcile Bell's inequality, QM, and the EPR paradox? Some
physicists and philosophers have put a lot of thought into these questions,
and even still carry out various Bell-type experiments;
indeed this still seems to be an area of somewhat active research.
There are many thorough analyses of the assumptions going into Bell's
theorem, one of which is Ref.~\cite{platoStanfordBell}. Here I wish
to focus on two possible resolutions, one because it is
easy to understand, and another because it is philosophically interesting.

Generally we choose to accept experimental results, because the way we
learn things is through observation. In my view, and probably in the view
of an overwhelming majority of physicists, observation is ultimately
our source of knowledge. Therefore we should accept the experimental
results and assume QM is correct. Hence one of the assumptions going into
Bell's theorem must be rejected.

\subsection{No local hidden variables}

One resolution, and I guess this is the most widely accepted resolution,
is to exclude local hidden variable theories. Under this interpretation,
physics really is fundamentally stochastic, and there is no missing
information. If there is no hidden variable, then wave function collapse
must be actually instantaneous to conserve angular momentum, and the
theory is in this sense fundamentally non-local. The question is thus:
Does this violate special relativity?

The requirement that influences not travel faster than light comes from
the demand that causality be preserved. So in particular, any
superluminal influence or motion that does not violate causality is
permitted. An easy example of such motion is the propagation of a shadow.
We imagine a person standing still with a light source moving in a semi-circle
around them, thereby casting a shadow on a distant wall. The apparent
speed of the shadow on the wall can be made arbitrarily large by putting
the way arbitrarily far away. This does not violate causality because the
no event in the shadow at the starting point causes any event in the
shadow at the ending point. The moving light source causes both shadows,
but at the speed of light, which is still allowed.

It remains to argue that instantaneous wave function collapse does not violate
causality. To see this, note that the only way that a particle's state
can influence anything in its neighborhood is, by definition, to take a
measurement. Furthermore according to QM, one can never know what the
outcome of a single measurement will be. These two facts
\begin{enumerate}
  \item guarantee that whatever happens in, say, the positron neighborhood
        requires first a measurement by the positron experimenter, so that
        then anything resulting from that measurement is guaranteed to
        be at a time-like separation from the measurement, and therefore 
        not violate causality; and
  \item guarantee that the experimenter at, say, the electron side cannot
        cause anything to happen on the positron side, because this
        experimenter cannot control their own outcome.
\end{enumerate}
More succinctly, the electron experimenter cannot cause the positron
measurement to happen, nor can this experimenter choose any outcome.
Hence no causal, superluminal influence can propagate between 
these two space-time locations.

\subsection{Superdeterminism}

Another way to proceed is to reject independence, i.e. to reject
eq. \eqref{eq:bellindep}. One could imagine\footnote{This is not necessarily
the only way to reject independence.}, for example, that there
is a region, common to the reverse light cones of the event ``choosing the
setting $\vec{b}$" and the event ``the result of the electron measurement
is $A$", in which something determines both the setting $\vec{b}$ and
the outcome $A$. Taken to its extreme, one could imagine that {\it all}
experimental outcomes are somehow determined, and could, in principle, be
traced back all the way to one set of initial conditions. Theories like
this are sometimes called {\it superdeterministic}.

In superdeterminism, QM would be viewed as an effective or emergent theory
with some completely deterministic theory underlying it. Hossenfelder
and Palmer \cite{hossenfelder_rethinking_2020} make some arguments for
why one should consider superdeterministic theories, explain some features
of a class of superdeterministic theories they would like to consider, and
address some common counterarguments to such theories. One of their arguments
for considering such theories, besides better understanding some of these
strange features of QM, is that physics right now is not yet complete,
and they hope that perhaps superdeterministic theories can fill at least
some of the gaps that current physics leaves void.

One thing which is interesting about superdeterminism is, if it's true,
it seems to formally rule out free will. Of course, free will was never
guaranteed by our current theories of physics anyway; indeed if all your
actions are determined by stochastic wave function collapses that 
propagate up to some emergent behavior, you are not really making
choices, at least not in the colloquial sense.

If there is some underlying superdeterministic theory, there is no guarantee
that it is discoverable. To me, things like Heisenberg's uncertainty relations
and G\"odel's incompleteness theorems seem to suggest that there are
fundamental limitations on our knowledge, inherent to the very nature of
the universe and, even worse, to logic. So the possibility that such theories
are forever out of our reach would not be surprising. Of course, a
superdeterministic theory doesn't have to be complete in that sense, and 
at least for the time being, it seems worthwhile to see if one can cook up
some theory that can at least explain {\it more} than what our current
physics explains, but not everything.


\bibliographystyle{unsrtnat}
\bibliography{bibliography}

